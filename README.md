# PTSD-Classification-TensorFlow
DeepLearning model that categorizes Tweets into depression or no depression categories and is intended to send webpage recommendations to those identified with depression.

Leveraging Twitter Activity for Accurate Identification of Depression and PTSD: A Deep Learning Approach
Social media platforms have become popular for users to express their feelings and share mental health-related experiences. Researchers have conducted numerous studies to analyze user-generated content on platforms like Twitter, Facebook, and Instagram to observe users' emotional states and identify mental illnesses. However, there is a need to explore further the potential of Twitter tweets in identifying mental disorders, specifically depression and post-traumatic stress disorder (PTSD).
Kaggle provides a file that collected 1.6 million Twitter posts from users who have self-reported diagnoses of depression and PTSD. By leveraging these posts' linguistic and emotional characteristics, we sought to develop a deep learning model capable of accurately identifying users' mental disorders based on their Twitter activity.
We employed a deep learning model founded on recurrent neural network (RNN) architecture with Long Short-Term Memory (LSTM) units to accomplish this. RNNs are well-suited for modeling time-sequential information, making them suitable for capturing the sequential nature of emotional states in conversational text data. LSTM units address the vanishing gradient problem, enabling effective processing of long-term dependencies.
Through this research, we aimed to contribute to the mental health analysis on social media by exploring the potential of Twitter tweets in identifying users' mental disorders, focusing on depression and PTSD. By analyzing user-generated content and leveraging deep learning techniques, we sought to develop a model capable of accurately classifying mental health conditions based on the linguistic and emotional characteristics of the posts.
 
Overview of Relevant Packages for NLP and Machine Learning
NLTK (Natural Language Toolkit):
NLTK is a thorough Python library for natural language processing (NLP) chores. It provides a wide range of modules and functions specifically designed to handle various aspects of NLP. These functionalities include tokenization, stemming, lemmatization, part-of-speech tagging, syntactic parsing, and more. (Jablonski, n.d.) NLTK also offers access to diverse corpora and lexicons that are valuable resources for NLP tasks. It is commonly employed for text preprocessing, feature engineering, and other related tasks within NLP pipelines.
Scikit-learn (sklearn):
Scikit-learn is a widespread machine-learning Python library widely used for various tasks. It provides a rich collection of machine-learning algorithms and tools, making it suitable for classification, regression, clustering, and dimensionality reduction tasks. Scikit-learn offers utilities for data preprocessing, feature extraction, model evaluation, and model selection. (Vickery, 2020) It simplifies building and evaluating machine-learning models by providing a consistent and user-friendly interface.
TensorFlow:
TensorFlow is an open-source machine-learning framework. It is designed to facilitate the creation and deployment of machine-learning models. TensorFlow offers a flexible ecosystem that supports both low-level operations and high-level abstractions. It contains high-level APIs such as Keras, which streamlines the building of neural networks and deep learning models. TensorFlow is extensively used for many applications, including image and text classification, natural language processing, and more. (Nguyen, et al., 2019)
TensorBoard:
TensorBoard is a web-based visualization tool provided by TensorFlow. It is a powerful companion for monitoring and visualizing the training process and model performance. With TensorBoard, users can track metrics, visualize model architectures, and examine histograms. It is commonly used during training and evaluation of machine learning models. TensorBoard provides interactive visualizations that enable users to gain insights into the behavior of their models, debug issues, and optimize their machine-learning workflows. (Johnson D. , 2023)
NLTK aids in text preprocessing and feature engineering, while scikit-learn provides numerous machine-learning algorithms and utilities. With its high-level APIs like Keras, TensorFlow is particularly valuable for building and training neural network models, including deep learning models. TensorBoard, as a companion tool for TensorFlow, enables users to visualize and analyze the performance and behavior of their models throughout the training process. Together, these packages form a robust toolkit for tackling several machine-learning and NLP challenges.
Preprocessing Tweet Text for Mental Health Analysis n Social Media Platforms: Leveraging NLTK for Text Cleaning and Transformation
	The focus is on preprocessing tweet text to analyze social media platforms' mental health and emotional states. The preprocess_tweettext() function employs various techniques to clean and transform the tweet data. It begins by converting the text to lowercase and subsequently removes URLs and usernames using regular expressions. The Natural Language Toolkit (NLTK) is an extensively used library for natural language processing. (Kim, Lee, Park, & Han, 2020) Several NLTK modules were utilized to process and analyze text data. These modules include the WordNetLemmatizer, stopwords corpus, and word_tokenize.
	The WordNetLemmatizer module allowed lemmatization, which reduces words to their base or dictionary form. This step is useful for normalizing the text data and reducing the dimensionality of the vocabulary. (Mishra, 2021)
	The stopwords corpus provided a list of common words that are typically considered irrelevant in text analysis, such as "the," "is," and "and." (n.A., GeeksForGeeks, 2023) We could focus on the more informative and meaningful words by removing these stopwords from the text data. 
	The word_tokenizer module facilitates tokenization, which requires splitting text into individual words or tokens. This step is fundamental for further analysis, such as counting word frequencies or building models. (n.A., Educba, 2023) By leveraging these NLTK functionalities, the text data could be preprocessed and transformed, making it suitable for subsequent analysis or modeling tasks.
Modeling Emotional States in Text Data Using Deep Recurrent Neural Networks: Leveraging LSTM Units for Sequential Dependancy Capture
This work uses a deep recurrent neural network (RNN) with Long Short-Term Memory (LSTM) units to model emotional states represented as time-sequential words in text data. RNNs are well-suited for processing time-sequential information, making them suitable for capturing the sequential nature of emotional states in conversational text data. The choice of RNN with LSTM units is beneficial for modeling emotional states, enabling the model to capture the sequential dependencies in the text data. The LSTM units address the vanishing gradient problem, allowing the model to process long-term information effectively. (Uddin, Dysthe, Folstad, & Brandtzaeg, 2021)
The Sequential model is defined in the code, which allows for a linear stack of layers. The model contains an embedding, LSTM, and several fully connected (Dense) layers with different activation functions. The Embedding layer is responsible for learning the dense word embeddings from the input text. The LSTM layer processes the sequential data, capturing the context and dependencies between words in the text. The following Dense layer introduces nin-linearity and reduces the dimensionality of the data. (fchollet, 2020) The final Dense layer uses a sigmoid activation function to output a probability value between 0 and 1, indicating the input text's sentiment.
 
(Uddin, Dysthe, Folstad, & Brandtzaeg, 2021)
	The model is amassed with the Adam optimizer and binary cross-entropy loss function, suitable for binary classification tasks. The accuracy metric is also specified to screen the model's performance during training. (Simic, 2023)
	Callbacks such as ModelCheckpoint, EarlyStopping, and ReduceLROnPlateau are defined to save the best model, perform early stopping to prevent overfitting and reduce the learning rate if the validation loss plateaus, respectively. (Johnson J. , 2021) A TensorBoard callback is included to visualize the training progress and metrics.
	The model is trained using the fit function, where X_train and y_trai are the training data and labels, respectively. The batch_size is set to the length of X_train, indicating that the model will be trained without using batches. The training is performed for a specific number of epochs, and the validation data (X_test, y_test) is used to evaluate the model's training performance. (Shama, 2021) The defined callbacks are issued to the fit function to control the training process.
Model results
 
	The evaluation results of the model on the test set are as follows: The loss value is 0.5072, indicating the average error of the model's predictions compared to the true labels. The accuracy achieved is 0.7570, which means that the model accurately predicted mental health conditions in 75.70% of the cases. The precision score 0.7470 indicates that 74.70% of the predicted positive labels were true positives. The recall score of 0.7843 signifies that the model effectively identified 78.43% of the true positive instances. Finally, the F1-score, which reflects both precision and recall, is 0.7652, reflecting a balanced performance.
	The model demonstrated decent performance on the test set. It achieved a reasonably high accuracy, indicating its ability to predict mental health conditions based on tweet data correctly. The precision score highlights that a significant proportion of the predicted positive labels were true positives. The recall score indicates a good ability to capture the true positive instances. The F1-score presents a balanced measure of the model's performance, reflecting precision and recall. These results suggest that the model has the potential to identify mental health conditions based on tweet data. However, further analysis and fine-tuning are required to improve its performance.
 
Conclusion
	This paper focused on leveraging Twitter activity to accurately identify depression and post-traumatic stress disorder (PTSD) using a deep learning approach. By analyzing user-generated content on Twitter, we aimed to develop a deep-learning model capable of accurately classifying users' mental health conditions based on their tweet data.
	We employed a deep-learning model based on recurrent neural network (RNN) architecture with Long Short-Term Memory (LSTM) units. RNNs are well-suited for modeling time-sequential information, making them suitable for capturing the sequential nature of emotional states in conversational text data. LSTM units address the vanishing gradient problem, enabling effective processing of long-term dependencies.
	Using natural language processing techniques and machine-learning algorithms, we successfully preprocessed and transformed the tweet data, focusing on linguistic and emotional characteristics. The developed deep-learning model demonstrated promising performance in accurately identifying mental health conditions, specifically depression and PTSD.
	The evaluation results on the test set showed that the model achieved a reasonably high accuracy, precision, recall, and F1-score. These metrics indicate the model's ability to correctly predict mental health conditions based on tweet data, capturing a significant proportion of true positives and effectively identifying instances of depression and PTSD.
	The findings contribute to the mental health analysis field on social media platforms. By leveraging deep-learning techniques and analyzing user-generated content, we have demonstrated the potential of Twitter tweets in identifying users' mental disorders.
	However, it is important to note that further analysis and fine-tuning of the model are necessary to improve its performance. Future research could explore additional features, such as sentiment analysis, contextual information, or user metadata, to enhance the model's accuracy and robustness. Additionally, considering the ethical implications of using social media data for mental health analysis is crucial, including privacy concerns and the responsible use of sensitive information.
 
References
fchollet. (2020, April 12). Keras. Retrieved from The Sequential Model: https://keras.io/guides/sequential_model/
Jablonski, J. (n.d.). Real Python. Retrieved from Natural Language Processing With Python's NLTK Package: https://realpython.com/nltk-nlp-python/
Johnson, D. (2023, May 27). Guru99. Retrieved from TensorBoard Tutorial: TensorFlow Graph Visualization: https://www.guru99.com/tensorboard-tutorial.html
Johnson, J. (2021, February 13). TowardsDataScience. Retrieved from Keras Callbacks and How to Save Your Model from Overtraining: https://towardsdatascience.com/keras-callbacks-and-how-to-save-your-model-from-overtraining-244fc1de8608
Kim, J., Lee, J., Park, E., & Han, J. (2020). A deep learning model for detecting mental illness from user content on social media. Scientific Reports Volume 10, Article number: 11846.
Mishra, A. (2021, September 26). Medium. Retrieved from Lemmatization in NLP using WordNetLemmatizer: https://aparnamishra144.medium.com/lemmatization-in-nlp-using-wordnetlemmatizer-420a444a50d
n.A. (2023, March 29). Educba. Retrieved from NLTK word_tokenize: https://www.educba.com/nltk-word_tokenize/
n.A. (2023, May 16). GeeksForGeeks. Retrieved from Removing stop words with NLTK in Python: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/#
Nguyen, G., Dlugolinsky, S., Bobák, M., Tran, V., López García, Á., Heredia, I., . . . Hluchý , L. (2019). Machine Learning and Deep Learning Frameworks and Libraries for large-scale data Mining: a survey. Artificial Intelligence Review, 77–124.
Shama, P. (2021, January 6). Machine Learning Knowledge. Retrieved from Keras Model Training Functions – fit() vs fit_generator() vs train_on_batch(): https://machinelearningknowledge.ai/keras-model-training-functions-fit-vs-fit_generator-vs-train_on_batch/
Simic, M. (2023, March 16). Baeldung. Retrieved from ADAM Optimizer: https://www.baeldung.com/cs/adam-optimizer
Uddin, Z., Dysthe, K. K., Folstad, A., & Brandtzaeg, P. B. (2021). Deep learning for prediction of depressive symptoms in a large textual dataset, 34. Neural Computing and Applications, 721–744.
Vickery, R. (2020, September 22). Towards Data Science. Retrieved from A Beginners Guide to Scikit-Learn: https://towardsdatascience.com/a-beginners-guide-to-scikit-learn-14b7e51d71a4

